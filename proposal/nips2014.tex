% Proposal for Convex Latent Variable
% Author:
%   Jimmy Lin
%   Ian Yen

\documentclass{article} % For LaTeX2e
\usepackage{nips14submit_e,times}
\usepackage{hyperref}
\usepackage{url}
\usepackage{amssymb,amsmath,amsfonts,latexsym,mathtext}
%\documentstyle[nips14submit_09,times,art10]{article} % For LaTeX 2.09


\title{Project Proposal: Convex Latent Variable}

%{{{ Authors
\author{
Hippocampus\thanks{ Use footnote for providing further information
about author (webpage, alternative address)---\emph{not} for acknowledging
funding agencies.} \\
Department of Computer Science\\
Cranberry-Lemon University\\
Pittsburgh, PA 15213 \\
\texttt{hippo@cs.cranberry-lemon.edu} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\AND
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
\And
Coauthor \\
Affiliation \\
Address \\
\texttt{email} \\
(if needed)\\
}
%}}}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to \LaTeX{} to determine where to break
% the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
% puts 3 of 4 authors names on the first line, and the last on the second
% line, try using \AND instead of \And before the third author name.

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%{{{ Macros
\DeclareMathOperator*{\argmin}{arg\,min}

\newcommand{\argmax}[1]{\underset{#1}{\text{argmax} }}

\newcommand{\indicator}[1]{\mathbf{1}_{#1}}
\newcommand{\LTwoNorm}[1]{||#1||^{2}}
\newcommand{\sumn}{\sum_{n}}
\newcommand{\sumk}{\sum_{k}}

\newcommand{\wnk}{w_{nk}}
\newcommand{\wn}{\mathbf{w}_n}
\newcommand{\wone}{\mathbf{w}_1}
\newcommand{\wtwo}{\mathbf{w}_2}
\newcommand{\w}{\mathbf{w}}
\newcommand{\wbar}{\overline{\w}}
\newcommand{\wopt}{\mathbf{w^{*}}}
\newcommand{\wnbyn}{\mathbf{w}_{n\times n}}

\newcommand{\x}[1]{\mathbf{x}_{#1}}
\newcommand{\xn}{\mathbf{x}_n}
\newcommand{\muk}{\boldsymbol{\mu}_k}
\newcommand{\maxn}{ \underset{n}{\text{max}} }

\newcommand{\yone}{\mathbf{y}_1}
\newcommand{\ytwo}{\mathbf{y}_2}
\newcommand{\z}{\mathbf{z}}
\newcommand{\quadraterm}[1]{\frac{\rho}{2}\LTwoNorm{\w_{#1}-\z}}
\newcommand{\dualterm}[1]{\mathbf{y}_{#1}^{T}(\w_{#1}-\z)}

\newcommand{\minimize}[1]{ \underset{#1}{\text{Minimize}} }
\newcommand{\maximize}[1]{ \underset{#1}{\text{Maximize}} }
\newcommand{\subjectto}{ \text{subject to} }
\newcommand{\hs}{\hspace*{0.6cm}}

%}}}

%\nipsfinalcopy % Uncomment for camera-ready version

%%% TODO list:
%% 1. reason of applying group-lasso (one or zero)
%% 2.

\begin{document}

\maketitle

\begin{abstract}
    Add abstract here..
\end{abstract}

\section{Problem Formulation}
\subsection{Motivation}
Given a set of data entities $\x{1}, ..., \xn, ..., \x{N} $, we wish to
automatically cluster these entities without specifying the number of cluster
centroids. One possible approach is to regard these input entities as
potential cluster candidates and evaluate the "belonging matrix" $\wnbyn$, consisting
of latent variables $\wnk$ indicating the extent of one entity $\xn$ explained
by one potential cluster centroid candidate $\muk$. In this project, we
propose a method to compute the sparse latent variable with faster
convergence by decomposing the objective function derived by ADMM into two
separate optimization task.


\subsection{Overall Optimization Goal}
Formally, our goal is to achieve clustering with group-lasso regularization.
Hence, we formulate the target problem in terms of notations introduced above
as follows:

 \begin{align}
  \minimize{\w}
  \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
        + \lambda \sumk \maxn | \wnk |  \\ % group-lasso
  \subjectto
  \hs & \forall n,\ \sumk \wnk \leq 1 \\
  \hs & \forall n,\ k,\ \wnk \geq 0
 \end{align}

The limiting conditions listed above are jointly called simplex constraint.
Every entity must be assigned to some centroid with prob 1.
Every entity must have non-negative belonging index to each centroid candidate.

Note that the group-lasso can either be one or zero in the context that we
pick up the most promising centroid candidates from provided entities.

\subsection{ADMM}

We can rewrite the initial optimization goal as the following primal problem:

 \begin{align}
  \minimize{\wone, \wtwo, \z}
  \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
    + \quadraterm{1} \nonumber \\
    & \hs + \lambda \sumk \maxn | \wnk |
    + \quadraterm{2}
        \\ % group-lasso
  \subjectto
  \hs & \w_{1} = \z,\ \w_{2} = \z
 \end{align}

 Note that we separate $\w$ into two variables, $\wone$ and $\wtwo$
 respectively. This is to prepare it for latter optimization decomposition.

By dual decomposition, the problem can be further transformed as follows:

 \begin{align}
     \minimize{\wone, \wtwo, \z} \hs  \maximize{\yone, \ytwo}
   \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
   + \dualterm{1} + \quadraterm{1} \nonumber \\
    & \hs + \lambda \sumk \maxn | \wnk |  % group-lasso
   + \dualterm{2} + \quadraterm{2}
 \end{align}

 We continue by incorporting duality and derive the following optimization
 task:

  \begin{align}
     \maximize{\yone, \ytwo} \hs  \minimize{\wone, \wtwo, \z}
   \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
   + \dualterm{1} + \quadraterm{1} \nonumber \\
    & \hs + \lambda \sumk \maxn | \wnk |  % group-lasso
   + \dualterm{2} + \quadraterm{2}
 \end{align}

 \subsection{Optimization Decomposition} \label{separate}
\newcommand{\mutw}[1]{\mathbf{y}_{#1}^{T} \w_{#1}}

The first optimization task:
  \begin{align}
   \minimize{\wone}
   \hs & \frac{1}{2} \sumn \sumk \wnk \LTwoNorm{\xn - \muk}  % clustering loss
   + \mutw{1} + \quadraterm{1} \\
   \subjectto
   \hs & \forall n,\ \sumk \wnk \leq 1 \\
   \hs & \forall n,\ k,\ \wnk \geq 0
 \end{align}
 This subproblem can be solved by Frank-Wolf Algorithm. 

 The second optimization task:
 \begin{align} \label{second}
   \minimize{\wtwo}
   \hs & \lambda \sumk \maxn | \wnk |  % group-lasso
   + \mutw{2} + \quadraterm{2} 
 \end{align}

 The closed-form solution of second subproblem can be found through Blockwise
 Coordinate Descent Procedures (Han Liu.). 

\subsection{Overall Solution Procedure}
Repeat

1. Resolve $\wone$ and $\wtwo$ in separate subproblem formulated in section
\ref{separate}.

2. Update the combination indicator $\z$ by $\z = \frac{1}{2} (\wone + \wtwo)$

3. Refine the multiplier $\yone$ and $\ytwo$ with gradient update rule: 
\begin{align}
\yone = \yone - \alpha \cdot (\wone - \z) \\
\ytwo = \ytwo - \alpha \cdot (\wtwo - \z)
\end{align}

Until Convergence.

\subsection{Resolve first subproblem: Frank-Wolf Algorithm} 
\subsubsection{Single-variable case}
In the context of only one input instance, the optimization problem can be written as
\begin{align}
   \minimize{w}
   \hs & \frac{1}{2} w \cdot d^2 + y \cdot w +   % clustering loss
   \frac{\rho}{2} ||w - z||^2 \\
   \subjectto
   \hs & 0 \leq  w \leq 1
\end{align}
Note that the distance $d$ above should be zero. We can further rewrite above
optimization task as 
\begin{align}
   \minimize{w}
   \hs & \frac{\rho}{2} w^2 + (- \rho  z + y + \frac{1}{2} d^2) w + z^2 \\
   \subjectto
   \hs & 0 \leq  w \leq 1
\end{align}
As we can see the critical point of objective function is 
\begin{align}
    -\frac{- \rho  z + y + \frac{1}{2}d^2}{\rho} 
    = z - \frac{1}{\rho} y - \frac{d^2}{2\rho}
\end{align}


\subsection{Resolve second subproblem: Blockwise Coordinate Descent Procedures} 
According to Blockwise Coordinate Descent Procedures (Han Liu.), the way to
resolve the \eqref{second} optimization problem is first to seek solution $\wbar$ without
group-lasso \eqref{noglasso} as shown below. 

\begin{align} \label{noglasso}
   \minimize{\wtwo}
   \hs & \mutw{2} + \quadraterm{2} 
 \end{align}

\newcommand{\ms}{m^{*}}
Then ultimately find the closed-form solution in terms of $\wbar$. In
practice, we resolve it column by column. The formula for closed-form
solution $\wopt$ is given as follows:

\begin{align}
    \wopt = ...
\end{align}

where $\ms = \argmax{m} \frac{1}{m} \big(\sum_{i'}^{m} |\wopt^{(k_{i'})}| -
\lambda \big)$.

\subsubsection*{Acknowledgments}

\subsubsection*{References}
\small{

}

\end{document}
